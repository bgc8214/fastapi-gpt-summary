import os
from fastapi import FastAPI, HTTPException
from openai import OpenAI
from starlette.responses import StreamingResponse
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("ğŸš¨ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

app = FastAPI()
client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… POST ìš”ì²­ìœ¼ë¡œ GPT-3.5 Turbo ìš”ì•½ API (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
@app.post("/stream-summary/")
async def stream_summary(payload: dict):
    text = payload.get("text")
    if not text:
        raise HTTPException(status_code=400, detail="âŒ 'text' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def generate():
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": f"ì´ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜: {text}"}],
                stream=True  # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”
            )
            for chunk in response:  # âœ… `async for` ëŒ€ì‹  `for` ì‚¬ìš©
                if chunk.choices and chunk.choices[0].delta.content:
                    yield (chunk.choices[0].delta.content + "\n").encode("utf-8")
        except Exception as e:
            yield f"âŒ Error: {str(e)}".encode("utf-8")

    return StreamingResponse(generate(), media_type="text/event-stream")
